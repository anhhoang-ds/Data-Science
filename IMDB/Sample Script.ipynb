{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMDB Reviews Sentiment Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we import the required packages. It is necessary to first install the following packages:  \n",
    "  \n",
    "`pip install pandas`  \n",
    "`pip install numpy`  \n",
    "`pip install nltk`  \n",
    "  \n",
    "To install TensorFlow on CPU:  \n",
    "`pip install tensorflow`  \n",
    "To install TensorFlow on GPU:  \n",
    "`pip install tensorflow-gpu`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "\n",
    "from tensorflow.python.keras.layers import Dense, LSTM, BatchNormalization, Embedding, Bidirectional\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.stem import SnowballStemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read the data from .csv format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('datasets/train.csv')\n",
    "test = pd.read_csv('datasets/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['dataset'] = \"train\"\n",
    "test['dataset'] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2592</td>\n",
       "      <td>0</td>\n",
       "      <td>Un-bleeping-believable! Meg Ryan doesn't even ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18359</td>\n",
       "      <td>1</td>\n",
       "      <td>This is a extremely well-made film. The acting...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1040</td>\n",
       "      <td>0</td>\n",
       "      <td>Every once in a long while a movie will come a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17262</td>\n",
       "      <td>1</td>\n",
       "      <td>Name just says it all. I watched this movie wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9908</td>\n",
       "      <td>0</td>\n",
       "      <td>This movie succeeds at being one of the most u...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  labels                                               text\n",
       "0   2592       0  Un-bleeping-believable! Meg Ryan doesn't even ...\n",
       "1  18359       1  This is a extremely well-made film. The acting...\n",
       "2   1040       0  Every once in a long while a movie will come a...\n",
       "3  17262       1  Name just says it all. I watched this movie wi...\n",
       "4   9908       0  This movie succeeds at being one of the most u..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    1\n",
       "2    0\n",
       "3    1\n",
       "4    0\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['labels'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data into training, validation, and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_y = np.eye(2)[train['labels'][:20000]] # One-hot encode the labels\n",
    "val_y = np.eye(2)[train['labels'][20000:]] # One-hot encode the labels\n",
    "trn_txt = train.text[:20000]\n",
    "val_txt = train.text[20000:]\n",
    "tst_txt = test.text\n",
    "texts = np.hstack([trn_txt, val_txt, tst_txt]).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for cleaning text and performing stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(x):\n",
    "    re1 = re.compile(r'  +')\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    x = ' '.join([stemmer.stem(word) for word in str(x).split(' ')])\n",
    "    x = x.replace('#39;', \"'\").replace('amp;', '&').replace('#146;', \"'\").replace(\n",
    "        'nbsp;', ' ').replace('#36;', '$').replace('\\\\n', \"\\n\").replace('quot;', \"'\").replace(\n",
    "        '<br />', \"\\n\").replace('\\\\\"', '\"').replace('<unk>','u_n').replace(' @.@ ','.').replace(\n",
    "        ' @-@ ','-').replace('\\\\', ' \\\\ ')\n",
    "    return re1.sub(' ', html.unescape(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a extremely well-made film. The acting, script and camera-work are all first-rate. The music is good, too, though it is mostly early in the film, when things are still relatively cheery. There are no really superstars in the cast, though several faces will be familiar. The entire cast does an excellent job with the script.<br /><br />But it is hard to watch, because there is no good end to a situation like the one presented. It is now fashionable to blame the British for setting Hindus and Muslims against each other, and then cruelly separating them into two countries. There is some merit in this view, but it\\'s also true that no one forced Hindus and Muslims in the region to mistreat each other as they did around the time of partition. It seems more likely that the British simply saw the tensions between the religions and were clever enough to exploit them to their own ends.<br /><br />The result is that there is much cruelty and inhumanity in the situation and this is very unpleasant to remember and to see on the screen. But it is never painted as a black-and-white case. There is baseness and nobility on both sides, and also the hope for change in the younger generation.<br /><br />There is redemption of a sort, in the end, when Puro has to make a hard choice between a man who has ruined her life, but also truly loved her, and her family which has disowned her, then later come looking for her. But by that point, she has no option that is without great pain for her.<br /><br />This film carries the message that both Muslims and Hindus have their grave faults, and also that both can be dignified and caring people. The reality of partition makes that realisation all the more wrenching, since there can never be real reconciliation across the India/Pakistan border. In that sense, it is similar to \"Mr & Mrs Iyer\".<br /><br />In the end, we were glad to have seen the film, even though the resolution was heartbreaking. If the UK and US could deal with their own histories of racism with this kind of frankness, they would certainly be better off.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = [stem(txt) for txt in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text after stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'this is a extrem well-mad film. the acting, script and camera-work are all first-rate. the music is good, too, though it is most earli in the film, when thing are still relat cheery. there are no realli superstar in the cast, though sever face will be familiar. the entir cast doe an excel job with the script.\\n\\nbut it is hard to watch, becaus there is no good end to a situat like the one presented. it is now fashion to blame the british for set hindus and muslim against each other, and then cruelli separ them into two countries. there is some merit in this view, but it also true that no one forc hindus and muslim in the region to mistreat each other as they did around the time of partition. it seem more like that the british simpli saw the tension between the religion and were clever enough to exploit them to their own ends.\\n\\nthe result is that there is much cruelti and inhuman in the situat and this is veri unpleas to rememb and to see on the screen. but it is never paint as a black-and-whit case. there is base and nobil on both sides, and also the hope for chang in the younger generation.\\n\\nthere is redempt of a sort, in the end, when puro has to make a hard choic between a man who has ruin her life, but also truli love her, and her famili which has disown her, then later come look for her. but by that point, she has no option that is without great pain for her.\\n\\nthis film carri the messag that both muslim and hindus have their grave faults, and also that both can be dignifi and care people. the realiti of partit make that realis all the more wrenching, sinc there can never be real reconcili across the india/pakistan border. in that sense, it is similar to \"mr & mrs iyer\".\\n\\nin the end, we were glad to have seen the film, even though the resolut was heartbreaking. if the uk and us could deal with their own histori of racism with this kind of frankness, they would certain be better off.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summaries[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an integer token for each word and apply the tokenizer to the datasets. For more information on Tensorflow/Keras for text processing see:  \n",
    "https://keras.io/preprocessing/text/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_words = 5000\n",
    "t = Tokenizer(n_words)\n",
    "t.fit_on_texts(summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_seq = t.texts_to_sequences([stem(txt) for txt in trn_txt])\n",
    "val_seq = t.texts_to_sequences([stem(txt) for txt in val_txt])\n",
    "tst_seq = t.texts_to_sequences([stem(txt) for txt in tst_txt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only keep up to 300 words of the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 500\n",
    "trn_seq5 = np.array(pad_sequences(trn_seq, max_words))\n",
    "val_seq5 = np.array(pad_sequences(val_seq, max_words))\n",
    "tst_seq5 = np.array(pad_sequences(tst_seq, max_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 400\n",
    "trn_seq4 = np.array(pad_sequences(trn_seq, max_words))\n",
    "val_seq4 = np.array(pad_sequences(val_seq, max_words))\n",
    "tst_seq4 = np.array(pad_sequences(tst_seq, max_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 300\n",
    "trn_seq3 = np.array(pad_sequences(trn_seq, max_words))\n",
    "val_seq3 = np.array(pad_sequences(val_seq, max_words))\n",
    "tst_seq3 = np.array(pad_sequences(tst_seq, max_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the first sentence (converted to an array of integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1034,    1,  360,  165,  138,   32,  404,  298,   16,    1,  218,\n",
       "         17,    7,    6,  215,    5,   56,   94,   37,    6,   58,   47,\n",
       "         96,    5,    3,  786,   30,    1,   27,    7,    6,  156, 1133,\n",
       "          5, 1363,    1,  714,   15,  191,    2, 4049,  486,  276,   74,\n",
       "          2,  101, 1827,  100,   89,  114,   37,    6,   46, 2596,    8,\n",
       "         10,  366,   17,    7,   87,  303,   11,   58,   27,  510,    2,\n",
       "       4049,    8,    1, 3781,    5,  276,   74,   13,   34,  124,  192,\n",
       "          1,   49,    4,    7,  110,   51,   30,   11,    1,  714,  374,\n",
       "        217,    1, 1055,  209,    1, 2026,    2,   72,  915,  202,    5,\n",
       "       1402,  100,    5,   64,  201, 2696,    1,  659,    6,   11,   37,\n",
       "          6,   78,    2,    8,    1,  786,    2,   10,    6,   54, 3772,\n",
       "          5,  385,    2,    5,   53,   19,    1,  254,   17,    7,    6,\n",
       "        118, 1259,   13,    3,  316,    2, 4320,  431,   37,    6,  455,\n",
       "          2,   19,  204,    2,   87,    1,  290,   15,  407,    8,    1,\n",
       "       1130, 3987,   37,    6, 4824,    4,    3,  426,    8,    1,   96,\n",
       "         50,   44,    5,   48,    3,  215, 1135,  209,    3,  131,   33,\n",
       "         44, 1147,   40,  126,   17,   87,  412,   90,   40,    2,   40,\n",
       "        293,   63,   44,   40,  101,  323,  121,   76,   15,   40,   17,\n",
       "         31,   11,  178,   57,   44,   58, 4000,   11,    6,  214,   85,\n",
       "        647,   15,   40,   10,   14,  708,    1,  888,   11,  204, 4049,\n",
       "          2,   23,   64, 2071,    2,   87,   11,  204,   67,   20,    2,\n",
       "        359,  509,    1,  865,    4,   48,   11, 1951,   29,    1,   51,\n",
       "        257,   37,   67,  118,   20,  152,  635,    1, 2419, 2614,    8,\n",
       "         11,  969,    7,    6,  629,    5,  473, 1812,    8,    1,   96,\n",
       "         80,   72, 1173,    5,   23,  115,    1,   14,   59,  161,    1,\n",
       "       4001,   12,   43,    1, 2159,    2,  183,  106,  516,   16,   64,\n",
       "        201,  767,    4, 2597,   16,   10,  232,    4,   34,   61,  304,\n",
       "         20,  134,  127])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trn_seq[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Neural Network with Keras to predict sentiment from sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We represent each word as 64 numbers, put the sequence through an LSTM Neural Network. For more information see: https://keras.io/getting-started/sequential-model-guide/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "        Embedding(n_words, 64, input_length = max_words, input_shape=(max_words,)),\n",
    "        BatchNormalization(),\n",
    "        LSTM(64, dropout=0.3, recurrent_dropout=0.3),\n",
    "        BatchNormalization(),\n",
    "        Dense(2, activation = 'softmax')\n",
    "    ])\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = Adam(lr=.01), metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 400, 64)           320000    \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 400, 64)           256       \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 64)                33024     \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 353,666\n",
      "Trainable params: 353,410\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 312s 16ms/step - loss: 0.5618 - acc: 0.7043 - val_loss: 0.3794 - val_acc: 0.8296\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 317s 16ms/step - loss: 0.3723 - acc: 0.8365 - val_loss: 0.3686 - val_acc: 0.8450\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 337s 17ms/step - loss: 0.3363 - acc: 0.8565 - val_loss: 0.3863 - val_acc: 0.8420\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 386s 19ms/step - loss: 0.3162 - acc: 0.8681 - val_loss: 0.3718 - val_acc: 0.8480\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 386s 19ms/step - loss: 0.2980 - acc: 0.8795 - val_loss: 0.3544 - val_acc: 0.8560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras._impl.keras.callbacks.History at 0x2d0225f0e48>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trn_seq,\n",
    "          trn_y,\n",
    "          validation_data = [val_seq, val_y],\n",
    "          epochs = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the sentiment for each review in the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tst_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.10833202, 0.89166796],\n",
       "       [0.00667799, 0.99332196],\n",
       "       [0.04753473, 0.95246524],\n",
       "       ...,\n",
       "       [0.22214174, 0.77785826],\n",
       "       [0.987879  , 0.01212099],\n",
       "       [0.9819125 , 0.01808748]], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most likely to be negative sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I watched this movie with my boyfriend, an avid hip-hop fan and he was really really looking forward to catch the \"soul\" vibe the movie claimed to have. Boy, we were dead wrong. When I finished watching the movie I felt two things: remorse and relief. Remorse because I regretted wasting my time to watch this awful piece of dung, and relief because I watched it free on cable.<br /><br />This movie really really gives a bad name to black people, by putting so much awful stereotypes that I believe all smart black people everywhere has been trying to spell off. I\\'m Asian, and I feel very very sorry and sick for those who made this movie. What more to say? Bad writing, even worse acting, and horrible storyline.<br /><br />Even if you\\'re bored to death and has no other choice, don\\'t watch this movie. Seriously. The movie really has nothing to offer, except if you want to see things like minor illegal drinking, animal slain, women degradation, and overall: A REALLY REALLY BAD-OBNOXIOUS-SICKENING-AWFUL MOVIE. Yuck.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.text.iloc[np.argmax(preds[:,0])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Most likely to be positive sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.text.iloc[np.argmax(preds[:,1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try out some of our own reviews for a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_words(strings):\n",
    "    if type(strings) is str:\n",
    "        strings = [strings]\n",
    "    seq = np.array(pad_sequences(t.texts_to_sequences([stem(string) for string in strings]),max_words))\n",
    "    pred = model.predict(seq)\n",
    "    for i in range(len(strings)):\n",
    "        print(\"%s  |  Positive Sentiment: %2.f%%\" % (strings[i], pred[i][1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline sentiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_words('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_words(['I love this movie! Great film','This movie is boring and terrible...'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_words(['highly recommended','recommended','not recommended'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_words(['good','not good','bad'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_words(['fast pace','slow pace','very slow pace'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['labels'] = preds[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id','labels']].to_csv('predictions5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.89166796, 0.99332196, 0.95246524, ..., 0.77785826, 0.01212099,\n",
       "       0.01808748], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Conv + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "from keras.datasets import imdb\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution1D\n",
    "from keras.layers import MaxPooling1D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"sigmoid\", filters=64, kernel_size=3, padding=\"same\")`\n",
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 300, 64)           512000    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 300, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 150, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 200)               212000    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 736,754\n",
      "Trainable params: 736,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8000\n",
    "review_length = 300\n",
    "\n",
    "embedding_vector_length = 64\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size,\n",
    "                    embedding_vector_length,\n",
    "                    input_length=review_length))\n",
    "\n",
    "# Embedding layer feeds a vector of 64D into the convolutional layer.\n",
    "#   The output is consolidated through a max-pool layer before feeding\n",
    "#   it sequentially through to the LSTM for analysis. This should reduce\n",
    "#   training time of the net. The accuracy could improve as the spacial\n",
    "#   structure learning of a CNN are merged with the sequential learning\n",
    "#   of an LSTM.\n",
    "\n",
    "model.add(Convolution1D(nb_filter=64,\n",
    "                        filter_length=3,\n",
    "                        activation='sigmoid',\n",
    "                        border_mode='same'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Compile model and fit to data\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 300, 64)           512000    \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 300, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 150, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 200)               212000    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 736,754\n",
      "Trainable params: 736,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 359s 18ms/step - loss: 0.5196 - acc: 0.7135 - val_loss: 0.4137 - val_acc: 0.8122\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 362s 18ms/step - loss: 0.3567 - acc: 0.8451 - val_loss: 0.3451 - val_acc: 0.8502\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 344s 17ms/step - loss: 0.2801 - acc: 0.8879 - val_loss: 0.3324 - val_acc: 0.8594\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 370s 18ms/step - loss: 0.2485 - acc: 0.9035 - val_loss: 0.3294 - val_acc: 0.8621\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 13512s 676ms/step - loss: 0.2264 - acc: 0.9118 - val_loss: 0.3450 - val_acc: 0.8481\n",
      "5000/5000 [==============================] - 23s 5ms/step\n",
      "Accuracy 84.81%\n"
     ]
    }
   ],
   "source": [
    "# Compile model and fit to data\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(trn_seq3, trn_y, validation_data=(val_seq3, val_y),\n",
    "          nb_epoch=5, batch_size=32)\n",
    "\n",
    "# Display accuracy\n",
    "evaluation = model.evaluate(val_seq3, val_y)\n",
    "print(\"Accuracy %0.2f%%\" % (evaluation[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tst_seq3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"sigmoid\", filters=64, kernel_size=3, padding=\"same\")`\n",
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:22: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=2)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 400, 64)           512000    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 400, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 200, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 200)               212000    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 736,754\n",
      "Trainable params: 736,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 8000\n",
    "review_length = 400\n",
    "\n",
    "embedding_vector_length = 64\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size,\n",
    "                    embedding_vector_length,\n",
    "                    input_length=review_length))\n",
    "\n",
    "# Embedding layer feeds a vector of 64D into the convolutional layer.\n",
    "#   The output is consolidated through a max-pool layer before feeding\n",
    "#   it sequentially through to the LSTM for analysis. This should reduce\n",
    "#   training time of the net. The accuracy could improve as the spacial\n",
    "#   structure learning of a CNN are merged with the sequential learning\n",
    "#   of an LSTM.\n",
    "\n",
    "model.add(Convolution1D(nb_filter=64,\n",
    "                        filter_length=3,\n",
    "                        activation='sigmoid',\n",
    "                        border_mode='same'))\n",
    "model.add(MaxPooling1D(pool_length=2))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Compile model and fit to data\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_7 (Embedding)      (None, 400, 64)           512000    \n",
      "_________________________________________________________________\n",
      "conv1d_5 (Conv1D)            (None, 400, 64)           12352     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1 (None, 200, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 200)               212000    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 736,754\n",
      "Trainable params: 736,754\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 468s 23ms/step - loss: 0.5278 - acc: 0.7014 - val_loss: 0.4150 - val_acc: 0.8075\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 9710s 485ms/step - loss: 0.3246 - acc: 0.8660 - val_loss: 0.3308 - val_acc: 0.8631\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 461s 23ms/step - loss: 0.2597 - acc: 0.8968 - val_loss: 0.3193 - val_acc: 0.8706\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 461s 23ms/step - loss: 0.2262 - acc: 0.9118 - val_loss: 0.3404 - val_acc: 0.8706\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 459s 23ms/step - loss: 0.2080 - acc: 0.9172 - val_loss: 0.3337 - val_acc: 0.8670\n",
      "5000/5000 [==============================] - 30s 6ms/step\n",
      "Accuracy 86.70%\n"
     ]
    }
   ],
   "source": [
    "# Compile model and fit to data\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(trn_seq4, trn_y, validation_data=(val_seq4, val_y),\n",
    "          nb_epoch=5, batch_size=32)\n",
    "\n",
    "# Display accuracy\n",
    "evaluation = model.evaluate(val_seq4, val_y)\n",
    "print(\"Accuracy %0.2f%%\" % (evaluation[1] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM + Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_vector_length = 64\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size,\n",
    "                    embedding_vector_length,\n",
    "                    input_length=review_length))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(2, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_8 (Embedding)      (None, 400, 64)           512000    \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 400, 64)           0         \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 200)               212000    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 724,402\n",
      "Trainable params: 724,402\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Compile model and fit to data\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 464s 23ms/step - loss: 0.2961 - acc: 0.8760 - val_loss: 0.5033 - val_acc: 0.7954\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 588s 29ms/step - loss: 0.1947 - acc: 0.9252 - val_loss: 0.3606 - val_acc: 0.8754\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 545s 27ms/step - loss: 0.1542 - acc: 0.9434 - val_loss: 0.3808 - val_acc: 0.8714\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 540s 27ms/step - loss: 0.1343 - acc: 0.9506 - val_loss: 0.4045 - val_acc: 0.8694\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 517s 26ms/step - loss: 0.1070 - acc: 0.9618 - val_loss: 0.4742 - val_acc: 0.8655\n",
      "5000/5000 [==============================] - 33s 7ms/step\n",
      "Accuracy 86.55%\n"
     ]
    }
   ],
   "source": [
    "model.fit(trn_seq4, trn_y, validation_data=(val_seq4, val_y),\n",
    "          nb_epoch=5, batch_size=32)\n",
    "\n",
    "# Display accuracy\n",
    "evaluation = model.evaluate(val_seq4, val_y)\n",
    "print(\"Accuracy %0.2f%%\" % (evaluation[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"sigmoid\", filters=128, kernel_size=4, padding=\"same\")`\n",
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=4)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 400, 64)           512000    \n",
      "_________________________________________________________________\n",
      "conv1d_6 (Conv1D)            (None, 400, 128)          32896     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1 (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_9 (LSTM)                (None, 200)               263200    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 808,498\n",
      "Trainable params: 808,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\keras\\models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 218s 11ms/step - loss: 0.5441 - acc: 0.6872 - val_loss: 0.3810 - val_acc: 0.8370\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 218s 11ms/step - loss: 0.3318 - acc: 0.8637 - val_loss: 0.3453 - val_acc: 0.8605\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 214s 11ms/step - loss: 0.2687 - acc: 0.8940 - val_loss: 0.3270 - val_acc: 0.8662\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 213s 11ms/step - loss: 0.2298 - acc: 0.9118 - val_loss: 0.3167 - val_acc: 0.8671\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 215s 11ms/step - loss: 0.2030 - acc: 0.9244 - val_loss: 0.3354 - val_acc: 0.8693\n",
      "5000/5000 [==============================] - 15s 3ms/step\n",
      "Accuracy 86.93%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size,\n",
    "                    embedding_vector_length,\n",
    "                    input_length=review_length))\n",
    "\n",
    "# Embedding layer feeds a vector of 64D into the convolutional layer.\n",
    "#   The output is consolidated through a max-pool layer before feeding\n",
    "#   it sequentially through to the LSTM for analysis. This should reduce\n",
    "#   training time of the net. The accuracy could improve as the spacial\n",
    "#   structure learning of a CNN are merged with the sequential learning\n",
    "#   of an LSTM.\n",
    "\n",
    "model.add(Convolution1D(nb_filter=128,\n",
    "                        filter_length=4,\n",
    "                        activation='sigmoid',\n",
    "                        border_mode='same'))\n",
    "model.add(MaxPooling1D(pool_length=4))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Compile model and fit to data\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(trn_seq4, trn_y, validation_data=(val_seq4, val_y),\n",
    "          nb_epoch=5, batch_size=32)\n",
    "\n",
    "# Display accuracy\n",
    "evaluation = model.evaluate(val_seq4, val_y)\n",
    "print(\"Accuracy %0.2f%%\" % (evaluation[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "model.save_weights('model_weights_1.h5')\n",
    "\n",
    "# Save the model architecture\n",
    "with open('model_architecture_1.json', 'w') as f:\n",
    "    f.write(model.to_json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"sigmoid\", filters=128, kernel_size=4, padding=\"same\")`\n",
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:18: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=4)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 400, 64)           512000    \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 400, 128)          32896     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1 (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 200)               263200    \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 808,498\n",
      "Trainable params: 808,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/4\n",
      "20000/20000 [==============================] - 241s 12ms/step - loss: 0.5322 - acc: 0.7042 - val_loss: 0.4663 - val_acc: 0.7924\n",
      "Epoch 2/4\n",
      "20000/20000 [==============================] - 221s 11ms/step - loss: 0.3343 - acc: 0.8626 - val_loss: 0.3765 - val_acc: 0.8465\n",
      "Epoch 3/4\n",
      "20000/20000 [==============================] - 219s 11ms/step - loss: 0.2637 - acc: 0.8970 - val_loss: 0.3219 - val_acc: 0.8678\n",
      "Epoch 4/4\n",
      "20000/20000 [==============================] - 219s 11ms/step - loss: 0.2343 - acc: 0.9104 - val_loss: 0.3233 - val_acc: 0.8713\n",
      "5000/5000 [==============================] - 16s 3ms/step\n",
      "Accuracy 87.13%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size,\n",
    "                    embedding_vector_length,\n",
    "                    input_length=review_length))\n",
    "\n",
    "# Embedding layer feeds a vector of 64D into the convolutional layer.\n",
    "#   The output is consolidated through a max-pool layer before feeding\n",
    "#   it sequentially through to the LSTM for analysis. This should reduce\n",
    "#   training time of the net. The accuracy could improve as the spacial\n",
    "#   structure learning of a CNN are merged with the sequential learning\n",
    "#   of an LSTM.\n",
    "\n",
    "model.add(Convolution1D(nb_filter=128,\n",
    "                        filter_length=4,\n",
    "                        activation='sigmoid',\n",
    "                        border_mode='same'))\n",
    "model.add(MaxPooling1D(pool_length=4))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Compile model and fit to data\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(trn_seq4, trn_y, validation_data=(val_seq4, val_y),\n",
    "          epochs=4, batch_size=32)\n",
    "\n",
    "# Display accuracy\n",
    "evaluation = model.evaluate(val_seq4, val_y)\n",
    "print(\"Accuracy %0.2f%%\" % (evaluation[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:19: UserWarning: Update your `Conv1D` call to the Keras 2 API: `Conv1D(activation=\"sigmoid\", filters=128, kernel_size=4, padding=\"same\")`\n",
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:20: UserWarning: Update your `MaxPooling1D` call to the Keras 2 API: `MaxPooling1D(pool_size=4)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 500, 64)           512000    \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 500, 128)          32896     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1 (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 125, 128)          0         \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 200)               263200    \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 402       \n",
      "=================================================================\n",
      "Total params: 808,498\n",
      "Trainable params: 808,498\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/3\n",
      "20000/20000 [==============================] - 271s 14ms/step - loss: 0.5433 - acc: 0.6943 - val_loss: 0.3748 - val_acc: 0.8380\n",
      "Epoch 2/3\n",
      "20000/20000 [==============================] - 287s 14ms/step - loss: 0.3275 - acc: 0.8652 - val_loss: 0.3337 - val_acc: 0.8654\n",
      "Epoch 3/3\n",
      "20000/20000 [==============================] - 326s 16ms/step - loss: 0.2660 - acc: 0.8953 - val_loss: 0.3115 - val_acc: 0.8672\n",
      "5000/5000 [==============================] - 21s 4ms/step\n",
      "Accuracy 86.72%\n"
     ]
    }
   ],
   "source": [
    "review_length = 500\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(vocab_size,\n",
    "                    embedding_vector_length,\n",
    "                    input_length=review_length))\n",
    "\n",
    "# Embedding layer feeds a vector of 64D into the convolutional layer.\n",
    "#   The output is consolidated through a max-pool layer before feeding\n",
    "#   it sequentially through to the LSTM for analysis. This should reduce\n",
    "#   training time of the net. The accuracy could improve as the spacial\n",
    "#   structure learning of a CNN are merged with the sequential learning\n",
    "#   of an LSTM.\n",
    "\n",
    "model.add(Convolution1D(nb_filter=128,\n",
    "                        filter_length=4,\n",
    "                        activation='sigmoid',\n",
    "                        border_mode='same'))\n",
    "model.add(MaxPooling1D(pool_length=4))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(200))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "# Compile model and fit to data\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "model.fit(trn_seq5, trn_y, validation_data=(val_seq5, val_y),\n",
    "          epochs=3, batch_size=32)\n",
    "\n",
    "# Display accuracy\n",
    "evaluation = model.evaluate(val_seq5, val_y)\n",
    "print(\"Accuracy %0.2f%%\" % (evaluation[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model.predict(tst_seq5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['labels'] = preds[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[['id','labels']].to_csv('predictions6.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
