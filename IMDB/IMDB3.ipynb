{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datasets/train.csv')\n",
    "df_test = pd.read_csv('datasets/test.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Un-bleeping-believable! Meg Ryan doesn't even ...\n",
       "1    This is a extremely well-made film. The acting...\n",
       "2    Every once in a long while a movie will come a...\n",
       "3    Name just says it all. I watched this movie wi...\n",
       "4    This movie succeeds at being one of the most u...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train = df.loc[:, 'text']\n",
    "\n",
    "y_train = df.loc[:, 'labels']\n",
    "\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24995    More of a Frisbee like turtle with fangs that ...\n",
       "24996    It really is a great film (after being able to...\n",
       "24997    I see it when I was 12 year old and I dream to...\n",
       "24998    This film is an absolute disgrace! I thoroughl...\n",
       "24999    \"Big Fat Liar\" comes as a welcome -- shallow, ...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = df_test.loc[:, 'text']\n",
    "\n",
    "data_test.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For more info, see http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def get_tokens(inp_txt):\n",
    "    \n",
    "    ## Lower case: ABC -> abc\n",
    "    txt_lower = inp_txt.lower()\n",
    "  \n",
    "    ## Remove punctuations (!, ', \", ., :, ;, )\n",
    "    #txt_lower_nopunct = txt_lower.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    #print(txt_lower_nopunct)\n",
    "    \n",
    "    \n",
    "    ## Tokenize:\n",
    "    tokens = nltk.word_tokenize(txt_lower) #_nopunct)\n",
    "    #tokens = nltk.wordpunct_tokenize(txt_lower)\n",
    "    \n",
    "    ## remove stop-words:\n",
    "    tokens_filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    \n",
    "    ## stemming:\n",
    "    stems = [stemmer.stem(t) for t in tokens_filtered]\n",
    "    stems_nopunct = [s for s in stems if re.match('^[a-zA-Z]+$', s) is not None]\n",
    "    return (stems_nopunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'call', 'rose', 'name', 'would', 'smell', 'sweet']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_tokens(\"What's in a name? That which we call a rose by any other name would smell as sweet.\")\n",
    "\n",
    "## Note: you need to download punkt package in nltk:\n",
    "# import nltk\n",
    "# nltk.download(punkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='replace',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='ascii', sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function get_tokens at 0x0000020FF44B5730>,\n",
       "        use_idf=True, vocabulary=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    encoding = 'utf-8',\n",
    "    decode_error = 'replace',\n",
    "    strip_accents = 'ascii',\n",
    "    analyzer = 'word',\n",
    "    smooth_idf = True,\n",
    "    tokenizer = get_tokens\n",
    ")\n",
    "\n",
    "tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: ['call', 'name', 'question', 'rose', 'smell', 'would']\n",
      "Example1: [[0.35355339 0.70710678 0.         0.35355339 0.35355339 0.35355339]]\n",
      "Example2: [[0. 0. 1. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## Shakespear quote## Shake \n",
    "example_txt_1 = \"What's in a name? That which we call a rose by any other name would smell as sweet.\"\n",
    "example_txt_2 = \"To be, or not to be: that is the question.\"\n",
    "\n",
    "tfidf = tfidf.fit([example_txt_1 + example_txt_2])\n",
    "\n",
    "example1 = tfidf.transform([example_txt_1])\n",
    "example2 = tfidf.transform([example_txt_2])\n",
    "\n",
    "print('Features: %s' %tfidf.get_feature_names())\n",
    "print('Example1: %s' %example1.toarray())\n",
    "print('Example2: %s' %example2.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples N= 25000,  Number of features d= 75218\n",
      "Number of Test Documents: 25000,  Number of features: 75218\n"
     ]
    }
   ],
   "source": [
    "### Vectorizing the training set:\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                             stop_words='english')\n",
    "X_train = vectorizer.fit_transform(data_train)\n",
    "\n",
    "print(\"Number of samples N= %d,  Number of features d= %d\" % X_train.shape)\n",
    "\n",
    "\n",
    "### Transforming the test dataset:\n",
    "X_test = vectorizer.transform(data_test)\n",
    "\n",
    "print(\"Number of Test Documents: %d,  Number of features: %d\" %X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn import grid_search\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import datetime\n",
    "import gc # python's garbage collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## For more info, see http://www.cs.duke.edu/courses/spring14/compsci290/assignments/lab02.html\n",
    "\n",
    "stemmer = nltk.stem.porter.PorterStemmer()\n",
    "\n",
    "def get_tokens(inp_txt):\n",
    "    \n",
    "    ## Lower case: ABC -> abc\n",
    "    txt_lower = inp_txt.lower()\n",
    "  \n",
    "    ## Remove punctuations (!, ', \", ., :, ;, )\n",
    "    #txt_lower_nopunct = txt_lower.translate(string.maketrans(\"\",\"\"), string.punctuation)\n",
    "    #print(txt_lower_nopunct)\n",
    "    \n",
    "    \n",
    "    ## Tokenize:\n",
    "    tokens = nltk.word_tokenize(txt_lower) #_nopunct)\n",
    "    #tokens = nltk.wordpunct_tokenize(txt_lower)\n",
    "    \n",
    "    ## remove stop-words:\n",
    "    tokens_filtered = [w for w in tokens if not w in stopwords.words('english')]\n",
    "    \n",
    "    ## stemming:\n",
    "    stems = [stemmer.stem(t) for t in tokens_filtered]\n",
    "    stems_nopunct = [s for s in stems if re.match('^[a-zA-Z]+$', s) is not None]\n",
    "    return (stems_nopunct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 2)\n"
     ]
    }
   ],
   "source": [
    "## For the purpose of represntation\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import pipeline\n",
    "from mlxtend.preprocessing import DenseTransformer\n",
    "\n",
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    encoding = 'utf-8',\n",
    "    decode_error = 'replace',\n",
    "    strip_accents = 'ascii',\n",
    "    analyzer = 'word',\n",
    "    max_features = 100,\n",
    "    smooth_idf = True,\n",
    "    sublinear_tf=True,\n",
    "    max_df=0.5,\n",
    "    stop_words='english',\n",
    "    tokenizer = get_tokens\n",
    ")\n",
    "\n",
    "clf_pipe = pipeline.Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('densify', DenseTransformer()),\n",
    "    ('clf', RandomForestRegressor(n_estimators = 10))\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "rf_model = clf_pipe.fit(data_train[0:1000], y_train[0:1000])\n",
    "\n",
    "pred_rf = rf_model.predict(data_test[0:1000])\n",
    "\n",
    "pred_rf = np.vstack((df_test.loc[0:999, 'id'], pred_rf)).T\n",
    "\n",
    "print(pred_rf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8.8606e+04 5.0000e-01]\n",
      " [8.8639e+04 1.0000e+00]\n",
      " [7.6170e+04 3.0000e-01]\n",
      " [8.1803e+04 6.0000e-01]\n",
      " [9.3371e+04 6.0000e-01]\n",
      " [8.7360e+04 3.0000e-01]\n",
      " [8.8547e+04 6.0000e-01]\n",
      " [8.8215e+04 7.0000e-01]\n",
      " [9.6187e+04 7.0000e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(pred_rf[1:10,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training part started      (17:00:33.484394)\n",
      "Training part finished     (18:57:57.304814)\n",
      "Testin part finished       (20:46:01.110892)\n",
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import pipeline\n",
    "import datetime\n",
    "\n",
    "tfidf = sklearn.feature_extraction.text.TfidfVectorizer(\n",
    "    encoding = 'utf-8',\n",
    "    decode_error = 'replace',\n",
    "    strip_accents = 'ascii',\n",
    "    analyzer = 'word',\n",
    "    max_features = 10000,\n",
    "    smooth_idf = True,\n",
    "    sublinear_tf=True,\n",
    "    max_df=0.5,\n",
    "    stop_words='english',\n",
    "    tokenizer = get_tokens\n",
    ")\n",
    "\n",
    "clf_pipe = pipeline.Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('densify', DenseTransformer()),\n",
    "    ('clf', RandomForestClassifier(n_estimators = 10, criterion='gini'))\n",
    "])\n",
    "\n",
    "\n",
    "current_time = datetime.datetime.now().time().isoformat()\n",
    "print(\"Training part started      (%s)\" %(current_time))\n",
    "rf_model = clf_pipe.fit(data_train, y_train)\n",
    "current_time = datetime.datetime.now().time().isoformat()\n",
    "print(\"Training part finished     (%s)\" %(current_time))\n",
    "\n",
    "pred_rf = rf_model.predict(data_test)\n",
    "current_time = datetime.datetime.now().time().isoformat()\n",
    "print(\"Testin part finished       (%s)\" %(current_time))\n",
    "\n",
    "pred_rf = np.vstack((df_test.loc[:, 'id'], pred_rf)).T\n",
    "\n",
    "print(pred_rf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\Users\\namhpham\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn import grid_search\n",
    "from sklearn import cross_validation\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "import datetime\n",
    "import gc # python's garbage collector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-8ee7361d2907>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_inx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_inx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m         \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf_pipe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_inx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_inx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m         \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtest_inx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    246\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    247\u001b[0m         \"\"\"\n\u001b[1;32m--> 248\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    249\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    250\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 Xt, fitted_transformer = fit_transform_one_cached(\n\u001b[0;32m    212\u001b[0m                     \u001b[0mcloned_transformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                     **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    214\u001b[0m                 \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, weight, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m                        **fit_params):\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 581\u001b[1;33m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    582\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1379\u001b[0m             \u001b[0mTf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0midf\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mweighted\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mterm\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1380\u001b[0m         \"\"\"\n\u001b[1;32m-> 1381\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1382\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1383\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m         vocabulary, X = self._count_vocab(raw_documents,\n\u001b[1;32m--> 869\u001b[1;33m                                           self.fixed_vocabulary_)\n\u001b[0m\u001b[0;32m    870\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    871\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m    790\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 792\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    793\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    794\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(doc)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m             return lambda doc: self._word_ngrams(\n\u001b[1;32m--> 266\u001b[1;33m                 tokenize(preprocess(self.decode(doc))), stop_words)\n\u001b[0m\u001b[0;32m    267\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-b04ef85ad4e5>\u001b[0m in \u001b[0;36mget_tokens\u001b[1;34m(inp_txt)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m## remove stop-words:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mtokens_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m## stemming:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-b04ef85ad4e5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m## remove stop-words:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mtokens_filtered\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mw\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokens\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m## stemming:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mwords\u001b[1;34m(self, fileids, ignore_lines_startswith)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \"\"\"\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_lines_startswith\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         return [line for line in line_tokenize(self.raw(fileids))\n\u001b[0m\u001b[0;32m     23\u001b[0m                 if not line.startswith(ignore_lines_startswith)]\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36mraw\u001b[1;34m(self, fileids)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\wordlist.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     26\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fileids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfileids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfileids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\corpus\\reader\\api.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, file)\u001b[0m\n\u001b[0;32m    211\u001b[0m         \"\"\"\n\u001b[0;32m    212\u001b[0m         \u001b[0mencoding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_root\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    214\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mstream\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\data.py\u001b[0m in \u001b[0;36mopen\u001b[1;34m(self, encoding)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m         \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    329\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[0mstream\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSeekableUnicodeStreamReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20ff4779b00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "alpha_params = [0.01, 0.02, 0.05, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]\n",
    "#alpha_params = [0.01, 0.02]\n",
    "colors = [\"red\", \"green\", \"blue\", \"gray\", \"#656522\", \"#12FF14\", \"#B0B0B0\", \"#AA0505\", \"#0145FF\", \"#670566\"]\n",
    "#pointtypes = ['o', '^']\n",
    "linestyles = ['-', '--', '-.', ':', '-', '--', '-.', ':', '--', '-.']\n",
    "\n",
    "fig_roc = plt.figure(1, figsize=(10, 8))\n",
    "ax_roc = fig_roc.add_subplot(1, 1, 1)\n",
    "\n",
    "for param,col,ls in zip(alpha_params, colors, linestyles):\n",
    "    \n",
    "    clf_pipe = pipeline.Pipeline([\n",
    "        ('vect', tfidf),\n",
    "        ('clf', MultinomialNB(alpha=param))\n",
    "    ])\n",
    "\n",
    "    cv = cross_validation.StratifiedKFold(y_train, n_folds=5)\n",
    "    \n",
    "    auc_res = 0\n",
    "    xr = np.linspace(0, 1, 100)\n",
    "    tpr_interp = np.zeros(shape=xr.shape, dtype=float)\n",
    "    \n",
    "    for i, (train_inx, test_inx) in enumerate(cv):\n",
    "        model = clf_pipe.fit(data_train[train_inx], y_train[train_inx])\n",
    "        pred = model.predict_proba(data_train[test_inx])\n",
    "        \n",
    "        fpr, tpr, thresh = metrics.roc_curve(y_train[test_inx], pred[:, 1])\n",
    "        auc_res += metrics.auc(fpr, tpr)\n",
    "        \n",
    "        tpr_interp += scipy.interp(xr, fpr, tpr)\n",
    "\n",
    "    current_time = datetime.datetime.now().time().isoformat()\n",
    "    print(\"Alpha = %.2f   ---->   AUC = %.4f     (%s)\" %(param, auc_res/len(cv), current_time))\n",
    "    \n",
    "    tpr_interp /= len(cv)\n",
    "    line_new = plt.plot(xr, tpr_interp)\n",
    "    plt.setp(line_new, color=col, linewidth=3, linestyle=ls)\n",
    "    \n",
    "\n",
    "plt.plot([0, 1], [0, 1], '--', lw=4, color='gray')\n",
    "\n",
    "plt.setp(ax_roc.get_xticklabels(), rotation='horizontal', fontsize=16)\n",
    "plt.setp(ax_roc.get_yticklabels(), rotation='vertical', fontsize=16)\n",
    "plt.axis([-0.05, 1.05, -0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate', size=20)\n",
    "plt.ylabel('True Positive Rate', size=20)\n",
    "plt.title('Multinomial NB Classification using TF-IDF Features', size=20)\n",
    "plt.legend(alpha_params, loc='lower right', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000, 2)\n"
     ]
    }
   ],
   "source": [
    "clf_pipe = pipeline.Pipeline([\n",
    "    ('vect', tfidf),\n",
    "    ('clf', MultinomialNB(alpha=2.0))\n",
    "])\n",
    "\n",
    "final_model = clf_pipe.fit(data_train, y_train)\n",
    "#pred_multNB = final_model.predict_proba(data_test)\n",
    "\n",
    "pred_multNB = final_model.predict(data_test)\n",
    "\n",
    "pred_multNB = np.vstack((df_test.loc[:, 'id'], pred_multNB)).T\n",
    "\n",
    "print(pred_multNB.shape)\n",
    "\n",
    "#np.savetxt('../results/pred.multinomialNB.alpha_optimized.csv', pred_multNB, fmt='%s,%1d', delimiter=',', header='id,labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[98481,     1],\n",
       "       [88606,     1],\n",
       "       [88639,     1],\n",
       "       ...,\n",
       "       [97410,     1],\n",
       "       [84907,     0],\n",
       "       [99798,     0]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_multNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(pred_multNB,columns = ['id','labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv('submission_2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98481</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>88606</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>88639</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>76170</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>81803</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>93371</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>87360</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>88547</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>88215</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>96187</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>81251</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>98086</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>79688</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>90580</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>86756</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>77906</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>95742</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>77455</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>76454</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>92650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>98042</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>81107</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>83839</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>87324</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>84526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>93868</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>88605</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>84436</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>91698</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>82413</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24970</th>\n",
       "      <td>99514</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24971</th>\n",
       "      <td>93545</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24972</th>\n",
       "      <td>95595</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24973</th>\n",
       "      <td>96429</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24974</th>\n",
       "      <td>80626</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24975</th>\n",
       "      <td>86124</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24976</th>\n",
       "      <td>82752</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24977</th>\n",
       "      <td>85465</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24978</th>\n",
       "      <td>88625</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24979</th>\n",
       "      <td>92441</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24980</th>\n",
       "      <td>75001</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24981</th>\n",
       "      <td>75548</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24982</th>\n",
       "      <td>99755</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24983</th>\n",
       "      <td>93744</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24984</th>\n",
       "      <td>88297</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24985</th>\n",
       "      <td>91057</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24986</th>\n",
       "      <td>91473</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24987</th>\n",
       "      <td>98693</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24988</th>\n",
       "      <td>88779</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24989</th>\n",
       "      <td>91923</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24990</th>\n",
       "      <td>81798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24991</th>\n",
       "      <td>91692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24992</th>\n",
       "      <td>91863</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24993</th>\n",
       "      <td>80078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24994</th>\n",
       "      <td>83279</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>81386</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>95999</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>97410</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>84907</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>99798</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  labels\n",
       "0      98481       1\n",
       "1      88606       1\n",
       "2      88639       1\n",
       "3      76170       0\n",
       "4      81803       0\n",
       "5      93371       1\n",
       "6      87360       0\n",
       "7      88547       1\n",
       "8      88215       1\n",
       "9      96187       1\n",
       "10     81251       0\n",
       "11     98086       1\n",
       "12     79688       0\n",
       "13     90580       1\n",
       "14     86756       0\n",
       "15     77906       0\n",
       "16     95742       1\n",
       "17     77455       0\n",
       "18     76454       0\n",
       "19     92650       1\n",
       "20     98042       1\n",
       "21     81107       0\n",
       "22     83839       1\n",
       "23     87324       0\n",
       "24     84526       0\n",
       "25     93868       1\n",
       "26     88605       1\n",
       "27     84436       0\n",
       "28     91698       0\n",
       "29     82413       0\n",
       "...      ...     ...\n",
       "24970  99514       0\n",
       "24971  93545       0\n",
       "24972  95595       1\n",
       "24973  96429       1\n",
       "24974  80626       0\n",
       "24975  86124       0\n",
       "24976  82752       0\n",
       "24977  85465       1\n",
       "24978  88625       1\n",
       "24979  92441       1\n",
       "24980  75001       0\n",
       "24981  75548       0\n",
       "24982  99755       0\n",
       "24983  93744       1\n",
       "24984  88297       1\n",
       "24985  91057       0\n",
       "24986  91473       1\n",
       "24987  98693       1\n",
       "24988  88779       1\n",
       "24989  91923       1\n",
       "24990  81798       0\n",
       "24991  91692       1\n",
       "24992  91863       1\n",
       "24993  80078       0\n",
       "24994  83279       0\n",
       "24995  81386       0\n",
       "24996  95999       1\n",
       "24997  97410       1\n",
       "24998  84907       0\n",
       "24999  99798       0\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
