{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This document attempts to examine the topic of text mining within Python. There are two major approaches to text mining: Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA). There are three elements in order to conduct both LSA and LDA: 1) a corpus (a set of n document), 2) a vocabulary (a set of m words), and 3) a matrix of size n * m – known as the term-document frequency matrix (a representation of the occurrence of words in the document(s)). Nonetheless, we have our corpus (i.e. the Frozen reviews). From this, we can derive the vocabulary by identifying the distinct words in the corpus. Finally, from the documents within the corpus and the vocabulary, we can create the term-document matrix.\n",
    "\n",
    "# Dataset\n",
    "\n",
    "We have a dataset that contains the reviews for the movie \"Frozen\". It contains 736 reviews scraped from various websites such as IMDB and Rotten Tomatoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## import the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "# nltk.download()\n",
    "# use the last line to download the libraries for the dictionaries onto local machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dataset, which is a series which contains 736 reviews in the `Text` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=pd.read_sas(\"C:/Users/namhpham/Documents/Personal files/R workspace/frozentxt.sas7bdat\",encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>When people speak of their favorite Disney mov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.0</td>\n",
       "      <td>A lot of people criticize Frozen for what it i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.0</td>\n",
       "      <td>This is a huge movie, seriously huge. You can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.0</td>\n",
       "      <td>Frozen is a legitimately great film but also a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>The last time Disney adapted a Hans Christian ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id                                               Text\n",
       "0  1.0  When people speak of their favorite Disney mov...\n",
       "1  2.0  A lot of people criticize Frozen for what it i...\n",
       "2  3.0  This is a huge movie, seriously huge. You can ...\n",
       "3  4.0  Frozen is a legitimately great film but also a...\n",
       "4  5.0  The last time Disney adapted a Hans Christian ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(736, 2)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then combine the text into a list of words by splitting by spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#conver list to string\n",
    "corpus=df.iloc[:,1]\n",
    "doc_complete_string=''.join(\"'\"+ w +\"',\" for w in corpus)\n",
    "#convert into list\n",
    "doc_complete=doc_complete_string.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then observe the frequency of the first 50 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frequency: \n",
      "[('of', 4), ('of', 4), ('of', 4), ('of', 4), ('the', 3), ('the', 3), ('the', 3), ('Disney', 2), ('and', 2), ('to', 2), ('Disney', 2), ('some', 2), ('and', 2), ('some', 2), ('to', 2), (\"'When\", 1), ('people', 1), ('speak', 1), ('their', 1), ('favorite', 1), ('movies,', 1), ('big', 1), ('four', 1), ('Renaissance', 1), ('films', 1), ('Golden', 1), ('Age', 1), ('animation', 1), ('are', 1), ('likely', 1), ('be', 1), ('mentioned.', 1), ('The', 1), ('past', 1), ('decade', 1), ('has', 1), ('seen', 1), ('movies', 1), ('that', 1), ('were', 1), ('hit', 1), ('or', 1), ('miss.', 1), ('Some', 1), ('considered', 1), ('classics,', 1), ('forgotten', 1), ('close', 1), ('being', 1), ('classics', 1)]\n"
     ]
    }
   ],
   "source": [
    "wordfreq = [doc_complete[:50].count(w) for w in doc_complete[:50]]\n",
    "zipped=list(zip(doc_complete[:50],wordfreq))\n",
    "\n",
    "print (\"Frequency: \\n\" + str(sorted(zipped,key=lambda x: x[1],reverse=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that 'of' appears the most. However, this word gives us no insight into the topic. The same case applies to 'the' or 'to'. They are part of stopping words. As a result, our next step is to remove punctuation, special characters, and stop-words (i.e. a, the, as..). From here we can see the list of vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation) \n",
    "lemma = WordNetLemmatizer()\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = ''.join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "doc_clean = [clean(doc).split() for doc in doc_complete]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the two versions and see that the documents have been cleaned up of stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'When\", 'people', 'speak', 'of', 'their', 'favorite', 'Disney', 'movies,', 'the', 'big', 'four', 'of', 'the', 'Renaissance', 'and', 'films', 'of', 'the', 'Golden', 'Age', 'of', 'animation', 'are', 'likely', 'to', 'be', 'mentioned.', 'The', 'past', 'decade', 'has', 'seen', 'Disney', 'movies', 'that', 'were', 'hit', 'or', 'miss.', 'Some', 'considered', 'classics,', 'some', 'forgotten', 'and', 'some', 'close', 'to', 'being', 'classics']\n"
     ]
    }
   ],
   "source": [
    "print(doc_complete[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['when'], ['people'], ['speak'], [], [], ['favorite'], ['disney'], ['movie'], [], ['big'], ['four'], [], [], ['renaissance'], [], ['film'], [], [], ['golden'], ['age'], [], ['animation'], [], ['likely'], [], [], ['mentioned'], [], ['past'], ['decade'], [], ['seen'], ['disney'], ['movie'], [], [], ['hit'], [], ['miss'], [], ['considered'], ['classic'], [], ['forgotten'], [], [], ['close'], [], [], ['classic']]\n"
     ]
    }
   ],
   "source": [
    "print(doc_clean[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then load the corpus iterator and dictionary from `gensim` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index. dictionary = corpora.Dictionary(doc_clean)\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12361 unique tokens: ['when', 'people', 'speak', 'favorite', 'disney']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here the LSA and LDA will start to go their own ways; accordingly, we first start our analysis by focusing on LSA.\n",
    "\n",
    "# Building a  Latent Semantic Analysis model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Latent\" means hidden, concealed. \"Semantic\" refers to meaning. So LSA refers to hidden meaning of the word. The method aims to detect the hidden meaning of words based on their existence in a collection of document. For example, the word \"bank\" might refer to money or stream of water. \n",
    "\n",
    "Latent Semantic Analysis arose from the problem of how to find relevant documents from search words. The fundamental difficulty arises when we compare words to find relevant documents, because what we really want to do is compare the meanings or concepts behind the words. LSA attempts to solve this problem by mapping both words and documents into a “concept” space and doing the comparison in this space.\n",
    "\n",
    "This is similar to exploratory factor analysis, where factors are searched for within a sample dataset of variables. The built-in function within gensim library took care of building the model, we here choose 25 topics and 5 words in each topic for easier interpretation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsi = gensim.models.lsimodel.LsiModel(corpus=doc_term_matrix, id2word=dictionary, num_topics=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '-1.000*\"movie\" + -0.000*\"almost\" + -0.000*\"build\" + 0.000*\"quite\" + 0.000*\"im\"'),\n",
       " (1,\n",
       "  '-1.000*\"disney\" + -0.000*\"old\" + 0.000*\"beast\" + 0.000*\"tale\" + 0.000*\"look\"'),\n",
       " (2,\n",
       "  '-1.000*\"film\" + 0.000*\"new\" + 0.000*\"interesting\" + 0.000*\"fan\" + -0.000*\"find\"'),\n",
       " (3,\n",
       "  '1.000*\"elsa\" + 0.001*\"anna\" + 0.001*\"pretty\" + 0.000*\"elsas\" + 0.000*\"still\"'),\n",
       " (4,\n",
       "  '0.950*\"anna\" + -0.314*\"character\" + -0.001*\"elsa\" + -0.000*\"isnt\" + 0.000*\"young\"'),\n",
       " (5,\n",
       "  '0.950*\"character\" + 0.314*\"anna\" + 0.000*\"right\" + -0.000*\"king\" + 0.000*\"kristen\"'),\n",
       " (6,\n",
       "  '1.000*\"it\" + 0.001*\"never\" + -0.000*\"voice\" + -0.000*\"help\" + 0.000*\"know\"'),\n",
       " (7,\n",
       "  '-1.000*\"frozen\" + -0.001*\"love\" + 0.001*\"old\" + -0.001*\"bad\" + -0.000*\"look\"'),\n",
       " (8,\n",
       "  '-1.000*\"like\" + 0.002*\"love\" + -0.001*\"watch\" + 0.001*\"lot\" + -0.001*\"king\"'),\n",
       " (9,\n",
       "  '-1.000*\"love\" + -0.002*\"like\" + 0.001*\"frozen\" + -0.001*\"guy\" + 0.001*\"all\"'),\n",
       " (10,\n",
       "  '-1.000*\"one\" + 0.001*\"new\" + 0.001*\"look\" + 0.001*\"something\" + 0.001*\"whole\"'),\n",
       " (11,\n",
       "  '-1.000*\"song\" + -0.001*\"find\" + 0.001*\"kingdom\" + 0.001*\"different\" + -0.001*\"done\"'),\n",
       " (12,\n",
       "  '1.000*\"story\" + -0.001*\"look\" + 0.001*\"definitely\" + -0.001*\"must\" + 0.001*\"young\"'),\n",
       " (13,\n",
       "  '-1.000*\"sister\" + -0.002*\"want\" + 0.002*\"away\" + -0.001*\"never\" + 0.001*\"control\"'),\n",
       " (14,\n",
       "  '1.000*\"go\" + 0.010*\"good\" + -0.004*\"time\" + 0.003*\"power\" + -0.003*\"well\"'),\n",
       " (15,\n",
       "  '1.000*\"good\" + -0.010*\"go\" + 0.006*\"time\" + -0.006*\"power\" + 0.003*\"something\"'),\n",
       " (16,\n",
       "  '-0.981*\"power\" + -0.181*\"really\" + -0.037*\"see\" + -0.037*\"music\" + 0.036*\"animation\"'),\n",
       " (17,\n",
       "  '-0.996*\"animation\" + -0.079*\"time\" + -0.033*\"power\" + -0.012*\"really\" + 0.010*\"let\"'),\n",
       " (18,\n",
       "  '0.985*\"time\" + -0.116*\"princess\" + -0.080*\"let\" + -0.079*\"animation\" + -0.041*\"also\"'),\n",
       " (19,\n",
       "  '0.983*\"really\" + -0.181*\"power\" + -0.016*\"also\" + -0.008*\"see\" + -0.008*\"music\"'),\n",
       " (20,\n",
       "  '0.995*\"great\" + 0.072*\"amazing\" + 0.043*\"music\" + 0.038*\"would\" + -0.019*\"princess\"'),\n",
       " (21,\n",
       "  '0.999*\"would\" + -0.038*\"great\" + 0.009*\"animated\" + 0.008*\"power\" + 0.008*\"see\"'),\n",
       " (22,\n",
       "  '-0.992*\"much\" + 0.119*\"even\" + -0.019*\"also\" + 0.008*\"well\" + 0.007*\"princess\"'),\n",
       " (23,\n",
       "  '-0.991*\"even\" + -0.119*\"much\" + -0.025*\"well\" + 0.022*\"animated\" + 0.020*\"make\"'),\n",
       " (24,\n",
       "  '0.694*\"let\" + -0.551*\"princess\" + 0.327*\"also\" + -0.245*\"animated\" + 0.200*\"make\"')]"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(num_topics=25, num_words=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with LSA, this process generated the underlying concepts by using singular value decomposition of the term-document frequency matrix. We can choose the number of topics to display and number of words in each topic for easier understanding. For example, topic 20 indicates that the movie has good soundtrack(s). Topic 23 indicates reviewers' appreciation of the animation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a  Latent Dirichlet Allocation model\n",
    "\n",
    "To begin the comparison, let’s start off with how LDA’s creation was motivated. In doing so, we start with an assumption: all of the words within each Frozen review are exchangeable. Under this assumption, we find ourselves leveraging a bag-of-words approach: for each of your M documents (i.e. reviews), you will choose a topic (z), and you will choose N vocabulary words where each chosen word is selected independently from a multinomial distribution that’s conditioned on the topic, z. When you will have one topic per document: the model is referred to as a mixture of unigrams. Conversely, when you want to allow a document to have multiple topics, one option is to turn to a probabilistic latent semantic indexing model (pLSI).\n",
    "Under the pLSI model, for each word of each document, you will choose a topic (z); however, it is now selected from a multinomial distribution conditioned on the specific document. With a topic (z) chosen, you then choose a vocabulary word selected from a multinomial distribution that is conditioned on the topic, z. Interestingly, because z is chosen from a distribution conditioned on a specific document from the corpus, the number of parameters in the pLSI model will increase linearly as the number of documents grows – which doesn’t make much sense from a reduction perspective.\n",
    "As such, LDA was born as a hierarchical probabilistic generative approach that models a corpus by topics using probability distributions over the vocabulary. By leveraging a finite vocabulary from a corpus, a number of topics (K), smoothing parameters (α and β) at the corpus-level to adjust the fit of the model, and a prior distribution over document lengths, LDA creates random documents whose contents are a mixture of topics. Then, comparing a document to two topics at a time, LDA determines which topic is closer to the document – repeating this across all possible combinations of topics. This will give us which documents are most relevant to which topics – a slight spin form the approach of LSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Trainign LDA model on the document term matrix.\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=25, id2word = dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.113*\"good\" + 0.112*\"really\" + 0.067*\"queen\" + 0.026*\"build\" + 0.023*\"shes\" + 0.023*\"anything\"'),\n",
       " (1,\n",
       "  '0.043*\"child\" + 0.036*\"since\" + 0.034*\"day\" + 0.031*\"different\" + 0.027*\"twist\" + 0.027*\"doesnt\"'),\n",
       " (2,\n",
       "  '0.183*\"one\" + 0.104*\"animation\" + 0.097*\"go\" + 0.047*\"loved\" + 0.042*\"didnt\" + 0.041*\"bit\"'),\n",
       " (3,\n",
       "  '0.067*\"way\" + 0.052*\"made\" + 0.040*\"show\" + 0.035*\"voice\" + 0.029*\"without\" + 0.027*\"far\"'),\n",
       " (4,\n",
       "  '0.088*\"power\" + 0.070*\"watch\" + 0.034*\"back\" + 0.027*\"everything\" + 0.024*\"kingdom\" + 0.023*\"sven\"'),\n",
       " (5,\n",
       "  '0.210*\"it\" + 0.065*\"think\" + 0.061*\"let\" + 0.046*\"im\" + 0.040*\"kristoff\" + 0.029*\"right\"'),\n",
       " (6,\n",
       "  '0.071*\"even\" + 0.069*\"first\" + 0.037*\"funny\" + 0.035*\"still\" + 0.033*\"find\" + 0.026*\"point\"'),\n",
       " (7,\n",
       "  '0.216*\"frozen\" + 0.196*\"love\" + 0.077*\"much\" + 0.061*\"little\" + 0.044*\"want\" + 0.029*\"look\"'),\n",
       " (8,\n",
       "  '0.086*\"snowman\" + 0.068*\"get\" + 0.063*\"classic\" + 0.052*\"han\" + 0.029*\"feature\" + 0.024*\"score\"'),\n",
       " (9,\n",
       "  '0.116*\"sister\" + 0.070*\"kid\" + 0.047*\"seen\" + 0.038*\"give\" + 0.029*\"adult\" + 0.028*\"cartoon\"'),\n",
       " (10,\n",
       "  '0.098*\"ice\" + 0.048*\"better\" + 0.045*\"could\" + 0.033*\"interesting\" + 0.025*\"important\" + 0.022*\"visuals\"'),\n",
       " (11,\n",
       "  '0.207*\"character\" + 0.073*\"make\" + 0.040*\"know\" + 0.032*\"tale\" + 0.025*\"world\" + 0.023*\"comedy\"'),\n",
       " (12,\n",
       "  '0.221*\"elsa\" + 0.193*\"like\" + 0.076*\"princess\" + 0.050*\"king\" + 0.026*\"main\" + 0.025*\"long\"'),\n",
       " (13,\n",
       "  '0.122*\"see\" + 0.050*\"feel\" + 0.037*\"u\" + 0.035*\"come\" + 0.022*\"work\" + 0.020*\"10\"'),\n",
       " (14,\n",
       "  '0.352*\"disney\" + 0.101*\"snow\" + 0.038*\"me\" + 0.028*\"course\" + 0.023*\"le\" + 0.019*\"rather\"'),\n",
       " (15,\n",
       "  '0.265*\"film\" + 0.221*\"anna\" + 0.112*\"time\" + 0.031*\"prince\" + 0.026*\"soundtrack\" + 0.023*\"put\"'),\n",
       " (16,\n",
       "  '0.147*\"story\" + 0.033*\"saw\" + 0.033*\"wonderful\" + 0.028*\"trailer\" + 0.025*\"thats\" + 0.023*\"original\"'),\n",
       " (17,\n",
       "  '0.080*\"would\" + 0.078*\"animated\" + 0.054*\"say\" + 0.040*\"end\" + 0.037*\"definitely\" + 0.032*\"beauty\"'),\n",
       " (18,\n",
       "  '0.108*\"best\" + 0.087*\"music\" + 0.041*\"the\" + 0.038*\"take\" + 0.035*\"singing\" + 0.030*\"favorite\"'),\n",
       " (19,\n",
       "  '0.053*\"family\" + 0.051*\"plot\" + 0.039*\"never\" + 0.035*\"though\" + 0.035*\"actually\" + 0.028*\"new\"'),\n",
       " (20,\n",
       "  '0.456*\"movie\" + 0.054*\"amazing\" + 0.041*\"something\" + 0.039*\"away\" + 0.038*\"old\" + 0.037*\"fun\"'),\n",
       " (21,\n",
       "  '0.174*\"song\" + 0.063*\"beautiful\" + 0.056*\"two\" + 0.048*\"watching\" + 0.031*\"every\" + 0.028*\"3d\"'),\n",
       " (22,\n",
       "  '0.088*\"also\" + 0.078*\"people\" + 0.076*\"well\" + 0.074*\"year\" + 0.068*\"dont\" + 0.051*\"musical\"'),\n",
       " (23,\n",
       "  '0.106*\"great\" + 0.072*\"thing\" + 0.047*\"lot\" + 0.047*\"ever\" + 0.030*\"girl\" + 0.029*\"heart\"'),\n",
       " (24,\n",
       "  '0.087*\"olaf\" + 0.052*\"tangled\" + 0.039*\"scene\" + 0.032*\"lion\" + 0.030*\"life\" + 0.025*\"perfect\"')]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldamodel.print_topics(num_topics=25, num_words=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the result in LSA, the result in LDA also includes the topic with some of the keywords. For example, topic 9 can be interpreted as a recommendation for Frozen as a good cartoon for adult. Topic 13 talks about the lyric of the main soundtrack. Topic 21 commends the songs in the movie. \n",
    "\n",
    "# Conclusion\n",
    "\n",
    "LSA calculates term/topic and document/topic matrices with associated loadings (between -1 and 1) to illustrate the shared semantic vector space. LDA calculates the same matrices, but populates them with something different: probabilities (ranging from 0 to 1) instead of loadings (i.e. correlations).\n",
    "Furthermore, while both techniques deliver reduction in the form of topics, LSA relies a factor-analysis-like approach that seeks to uncover latent structures in the corpus by leveraging linear algebra and transpositions. LDA, on the other hand, takes a Bayesian approach that started, instead, with potential structure and attempts to see which words stick.\n",
    "There's not one method that always works better than another. As a result, we would need to come to problem definition step at the beginning and need to figure out what we really need to achieve, and use the techniques to tell a compelling story."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
